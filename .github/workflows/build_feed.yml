name: Build filtered CSV feed + Amazon feeds (multi-country)

on:
  schedule:
    - cron: "*/15 * * * *"     # filtro + diff + amazon feeds
    - cron: "5 0 * * *"        # baseline giornaliera (UTC)
  workflow_dispatch:

concurrency:
  group: feed-filter
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Debug repo files
        run: |
          set -e
          echo "PWD=$(pwd)"
          ls -lah
          echo "Find key scripts:"
          find . -maxdepth 2 -type f -name "generate_amazon_feeds.py" -o -name "filter_feed.py" -o -name "upload_to_drive.py" -o -name "publish_spapi_feed.py" -print

      # ===============================
      # INSTALL DEPS
      # ===============================
      - name: Install Python deps (Google + SP-API)
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install google-api-python-client google-auth google-auth-httplib2
          pip install requests botocore

      # ===============================
      # DOWNLOAD SOURCE
      # ===============================
      - name: Download source CSV
        run: |
          set -e
          echo "Downloading source..."
          curl -L --retry 5 --retry-delay 5 --connect-timeout 30 --max-time 3600 \
            -w "\nHTTP_CODE=%{http_code}\nFINAL_URL=%{url_effective}\n" \
            "${{ secrets.SOURCE_CSV_URL }}" -o source.csv
          echo "Downloaded file:"
          ls -lh source.csv
          echo "Header preview:"
          head -n 2 source.csv | cat

      # ===============================
      # FILTER
      # ===============================
      - name: Filter CSV
        run: |
          set -e
          python filter_feed.py
          echo "Filtered file:"
          ls -lh filtered.csv
          echo "Filtered header preview:"
          head -n 2 filtered.csv | cat

      # ===============================
      # SERVICE ACCOUNT (Drive + Sheets)
      # ===============================
      - name: Write service account JSON to file
        env:
          GDRIVE_SA_JSON: ${{ secrets.GDRIVE_SA_JSON }}
        run: |
          set -e
          if [ -z "$GDRIVE_SA_JSON" ]; then
            echo "GDRIVE_SA_JSON secret is empty or missing"
            exit 1
          fi
          echo "$GDRIVE_SA_JSON" > sa.json
          python - << 'PY'
          import json
          with open("sa.json","r",encoding="utf-8") as f:
              obj = json.load(f)
          print("SA client_email:", obj.get("client_email"))
          PY

      # ===============================
      # UPLOAD filtered.csv -> Google Sheet "Filtered" (columns preserved)
      # (requires upload_csv_to_sheet.py in repo + secret FILTERED_SHEET_ID)
      # ===============================
      - name: Upload filtered.csv to Google Sheet "Filtered"
        env:
          FILTERED_SHEET_ID: ${{ secrets.FILTERED_SHEET_ID }}
          INPUT_CSV: filtered.csv
          FILTERED_SHEET_NAME: Filtered
        run: |
          set -e
          python upload_csv_to_sheet.py


      # ===============================
      # BASELINE: daily reset (UTC)
      # ===============================
      - name: Build baseline_skus.txt (daily)
        if: github.event_name == 'schedule' && github.event.schedule == '5 0 * * *'
        run: |
          set -e
          python - << 'PY'
          import csv
          from pathlib import Path

          def detect_delim(text: str) -> str:
              sample = text[:8192]
              try:
                  d = csv.Sniffer().sniff(sample, delimiters=",;\t|")
                  return d.delimiter
              except Exception:
                  first = text.splitlines()[0] if text else ""
                  return ";" if first.count(";") > first.count(",") else ","

          def find_col(header_l, candidates):
              norm = [h.replace(" ", "_") for h in header_l]
              for c in candidates:
                  c0 = c.lower().replace(" ", "_")
                  for i, h in enumerate(norm):
                      if h == c0:
                          return i
                  for i, h in enumerate(norm):
                      if c0 in h:
                          return i
              return -1

          raw = Path("filtered.csv").read_bytes()
          text = raw.decode("utf-8-sig", errors="replace")
          delim = detect_delim(text)
          reader = csv.reader(text.splitlines(), delimiter=delim)

          header = next(reader, [])
          header_l = [h.strip().lower() for h in header]
          sku_i = find_col(header_l, ["sku","seller_sku","seller sku","item_sku","item sku","merchant_sku","merchant sku","sku_id"])
          if sku_i < 0:
              raise RuntimeError(f"SKU column not found. Detected delimiter={delim!r}. Header={header}")

          seen = set()
          for row in reader:
              if not row or sku_i >= len(row):
                  continue
              sku = (row[sku_i] or "").strip()
              if sku:
                  seen.add(sku)

          with open("baseline_skus.txt","w",encoding="utf-8",newline="\n") as out:
              for sku in sorted(seen):
                  out.write(sku + "\n")

          print("Detected delimiter:", repr(delim))
          print("Baseline SKUs:", len(seen))
          PY

      - name: Upload baseline_skus.txt to Google Drive (overwrite by fileId)
        if: github.event_name == 'schedule' && github.event.schedule == '5 0 * * *'
        env:
          FILE_ID: ${{ secrets.BASELINE_FILE_ID }}
          LOCAL_FILE: baseline_skus.txt
        run: |
          set -e
          python upload_to_drive.py

      # ===============================
      # DIFF: every 15 min (and manual)
      # ===============================
      - name: Download baseline_skus.txt from Google Drive
        if: github.event_name != 'schedule' || github.event.schedule != '5 0 * * *'
        env:
          BASELINE_FILE_ID: ${{ secrets.BASELINE_FILE_ID }}
        run: |
          set -e
          python - << 'PY'
          import os
          from google.oauth2 import service_account
          from googleapiclient.discovery import build

          file_id = os.environ.get("BASELINE_FILE_ID","").strip()
          if not file_id:
              raise RuntimeError("BASELINE_FILE_ID is empty or missing")

          creds = service_account.Credentials.from_service_account_file(
              "sa.json",
              scopes=["https://www.googleapis.com/auth/drive"]
          )
          service = build("drive", "v3", credentials=creds)

          data = service.files().get_media(fileId=file_id).execute()
          with open("baseline_skus.txt","wb") as f:
              f.write(data)
          print("Downloaded baseline_skus.txt bytes:", len(data))
          PY

      - name: Build new_skus_0930.csv (diff vs baseline, same headers as filtered)
        if: github.event_name != 'schedule' || github.event.schedule != '5 0 * * *'
        run: |
          set -e
          python - << 'PY'
          import csv
          from pathlib import Path

          def detect_delim(text: str) -> str:
              sample = text[:8192]
              try:
                  d = csv.Sniffer().sniff(sample, delimiters=",;\t|")
                  return d.delimiter
              except Exception:
                  first = text.splitlines()[0] if text else ""
                  if "\t" in first: return "\t"
                  if "|" in first: return "|"
                  if ";" in first: return ";"
                  return ","

          def find_sku_field(fieldnames):
              if not fieldnames:
                  return None
              lower_map = {f.strip().lower(): f for f in fieldnames}
              for cand in ["sku","seller_sku","seller sku","item_sku","item sku","merchant_sku","merchant sku","sku_id"]:
                  if cand in lower_map:
                      return lower_map[cand]
              for f in fieldnames:
                  if "sku" in f.strip().lower():
                      return f
              return None

          # ---- baseline ----
          baseline = set()
          with open("baseline_skus.txt","r",encoding="utf-8",errors="ignore") as f:
              for line in f:
                  s = line.strip()
                  if s:
                      baseline.add(s)

          # ---- read filtered (detect delimiter from real file) ----
          with open("filtered.csv","r",encoding="utf-8-sig",newline="") as fin:
              first = fin.readline()
              fin.seek(0)
              if "\t" in first:
                  delim = "\t"
              elif "|" in first:
                  delim = "|"
              elif ";" in first:
                  delim = ";"
              else:
                  delim = ","

              reader = csv.DictReader(fin, delimiter=delim)
              fieldnames = reader.fieldnames
              if not fieldnames:
                  raise RuntimeError("filtered.csv has no header")

              sku_field = find_sku_field(fieldnames)
              if not sku_field:
                  raise RuntimeError(f"SKU column not found in header: {fieldnames}")

              rows_out = 0
              with open("new_skus_0930.csv","w",encoding="utf-8",newline="") as fout:
                  w = csv.DictWriter(fout, fieldnames=fieldnames)
                  w.writeheader()

                  for row in reader:
                      sku = (row.get(sku_field) or "").strip()
                      if not sku or sku in baseline:
                          continue
                      w.writerow(row)
                      rows_out += 1

          print("Detected delimiter:", repr(delim))
          print("Baseline size:", len(baseline))
          print("New SKUs rows:", rows_out)
          print("Header used:", fieldnames)
          PY

      # ===============================
      # UPLOAD new_skus_0930.csv -> Google Sheet "New_SKUs_0930"
      # (requires upload_csv_to_sheet.py + secret NEWSKUS_SHEET_ID)
      # ===============================
      - name: Upload new_skus_0930.csv to Google Sheet "New_SKUs_0930"
        if: github.event_name != 'schedule' || github.event.schedule != '5 0 * * *'
        env:
          FILTERED_SHEET_ID: ${{ secrets.NEWSKUS_SHEET_ID }}
          INPUT_CSV: new_skus_0930.csv
          FILTERED_SHEET_NAME: New_SKUs_0930
        run: |
          set -e
          python upload_csv_to_sheet.py

           # ===============================
      # GENERATE AMAZON FEEDS + SEND LISTINGS (MULTI-COUNTRY)
      # ===============================

      # ---------- IT ----------
      - name: Generate Amazon feeds IT
        env:
          GSHEET_ID: ${{ secrets.GSHEET_ID }}
          COUNTRY: it
          AMAZON_SELLER_ID: ${{ secrets.AMAZON_SELLER_ID }}
        run: |
          set -e
          python generate_amazon_feeds.py
          ls -lh amazon_it_*

      - name: Restore listings hash cache (IT)
        uses: actions/cache@v4
        with:
          path: .cache
          key: listings-hash-it-${{ github.ref_name }}
          restore-keys: |
            listings-hash-it-

      - name: Check if listings JSON changed (IT)
        id: check_it
        run: |
          set -e
          mkdir -p .cache
          FILE=amazon_it_listings.json
          if [ ! -f "$FILE" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          NEW_HASH=$(md5sum "$FILE" | awk '{print $1}')
          OLD_HASH=""
          [ -f .cache/amazon_it_listings.md5 ] && OLD_HASH=$(cat .cache/amazon_it_listings.md5 || true)
          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -n "$NEW_HASH" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "$NEW_HASH" > .cache/amazon_it_listings.md5
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Send Amazon IT Listings JSON feed (SP-API)
        if: steps.check_it.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          OUT=$(python publish_spapi_feed.py \
            --file amazon_it_listings.json \
            --feed-type JSON_LISTINGS_FEED \
            --marketplace APJ6JRA9NG5V4)
          echo "$OUT"
          FEED_ID=$(echo "$OUT" | grep -o '"feedId": *"[0-9]*"' | grep -o '[0-9]*' || true)
          test -n "$FEED_ID"
          echo "FEED_ID_IT=$FEED_ID" >> $GITHUB_ENV

      - name: Check feed processing status (IT)
        if: steps.check_it.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          sleep 20
          python check_spapi_feed.py --feed-id "$FEED_ID_IT"
          mv -f feed_status.json feed_status_it.json || true
          mv -f feed_processing_report.txt feed_processing_report_it.txt || true
          mv -f feed_document_meta.json feed_document_meta_it.json || true

      - name: Upload artifacts (IT)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: it-feed-artifacts
          path: |
            amazon_it_listings.json
            amazon_it_b2c.csv
            amazon_it_b2b.csv
            feed_status_it.json
            feed_processing_report_it.txt
            feed_document_meta_it.json
          if-no-files-found: warn
          retention-days: 7

      - name: Fail if feed report contains errors (IT)
        if: steps.check_it.outputs.changed == 'true'
        run: |
          set -e
          python - << 'PY'
          import json, sys, pathlib
          p = pathlib.Path("feed_processing_report_it.txt")
          if not p.exists():
              print("Missing report -> fail")
              sys.exit(1)
          txt = p.read_text(encoding="utf-8", errors="replace").strip()
          obj = json.loads(txt)
          summary = obj.get("summary") or {}
          errors = int(summary.get("errors") or 0)
          warnings = int(summary.get("warnings") or 0)
          print("Report summary IT:", "errors=", errors, "warnings=", warnings)
          if errors > 0:
              print("Issues:", obj.get("issues", [])[:50])
              sys.exit(1)
          PY

      - name: Upload amazon_it_listings.json to Drive (backup)
        if: always()
        env:
          FILE_ID: ${{ secrets.AMAZON_IT_LISTINGS_JSON_FILE_ID }}
          LOCAL_FILE: amazon_it_listings.json
        run: |
          set -e
          test -n "$FILE_ID"
          test -f "$LOCAL_FILE"
          python upload_to_drive.py


      # ---------- FR ----------
      - name: Generate Amazon feeds FR
        env:
          GSHEET_ID: ${{ secrets.GSHEET_ID }}
          COUNTRY: fr
          AMAZON_SELLER_ID: ${{ secrets.AMAZON_SELLER_ID }}
        run: |
          set -e
          python generate_amazon_feeds.py
          ls -lh amazon_fr_*

      - name: Restore listings hash cache (FR)
        uses: actions/cache@v4
        with:
          path: .cache
          key: listings-hash-fr-${{ github.ref_name }}
          restore-keys: |
            listings-hash-fr-

      - name: Check if listings JSON changed (FR)
        id: check_fr
        run: |
          set -e
          mkdir -p .cache
          FILE=amazon_fr_listings.json
          if [ ! -f "$FILE" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          NEW_HASH=$(md5sum "$FILE" | awk '{print $1}')
          OLD_HASH=""
          [ -f .cache/amazon_fr_listings.md5 ] && OLD_HASH=$(cat .cache/amazon_fr_listings.md5 || true)
          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -n "$NEW_HASH" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "$NEW_HASH" > .cache/amazon_fr_listings.md5
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Send Amazon FR Listings JSON feed (SP-API)
        if: steps.check_fr.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          OUT=$(python publish_spapi_feed.py \
            --file amazon_fr_listings.json \
            --feed-type JSON_LISTINGS_FEED \
            --marketplace A13V1IB3VIYZZH)
          echo "$OUT"
          FEED_ID=$(echo "$OUT" | grep -o '"feedId": *"[0-9]*"' | grep -o '[0-9]*' || true)
          test -n "$FEED_ID"
          echo "FEED_ID_FR=$FEED_ID" >> $GITHUB_ENV

      - name: Check feed processing status (FR)
        if: steps.check_fr.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          sleep 20
          python check_spapi_feed.py --feed-id "$FEED_ID_FR"
          mv -f feed_status.json feed_status_fr.json || true
          mv -f feed_processing_report.txt feed_processing_report_fr.txt || true
          mv -f feed_document_meta.json feed_document_meta_fr.json || true

      - name: Upload artifacts (FR)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: fr-feed-artifacts
          path: |
            amazon_fr_listings.json
            amazon_fr_b2c.csv
            amazon_fr_b2b.csv
            feed_status_fr.json
            feed_processing_report_fr.txt
            feed_document_meta_fr.json
          if-no-files-found: warn
          retention-days: 7

      - name: Fail if feed report contains errors (FR)
        if: steps.check_fr.outputs.changed == 'true'
        run: |
          set -e
          python - << 'PY'
          import json, sys, pathlib
          p = pathlib.Path("feed_processing_report_fr.txt")
          if not p.exists():
              print("Missing report -> fail")
              sys.exit(1)
          obj = json.loads(p.read_text(encoding="utf-8", errors="replace").strip())
          errors = int((obj.get("summary") or {}).get("errors") or 0)
          print("Report summary FR errors=", errors)
          if errors > 0:
              print("Issues:", obj.get("issues", [])[:50])
              sys.exit(1)
          PY

      - name: Upload amazon_fr_listings.json to Drive (backup)
        if: always()
        env:
          FILE_ID: ${{ secrets.AMAZON_FR_LISTINGS_JSON_FILE_ID }}
          LOCAL_FILE: amazon_fr_listings.json
        run: |
          set -e
          test -n "$FILE_ID"
          test -f "$LOCAL_FILE"
          python upload_to_drive.py


      # ---------- DE ----------
      - name: Generate Amazon feeds DE
        env:
          GSHEET_ID: ${{ secrets.GSHEET_ID }}
          COUNTRY: de
          AMAZON_SELLER_ID: ${{ secrets.AMAZON_SELLER_ID }}
        run: |
          set -e
          python generate_amazon_feeds.py
          ls -lh amazon_de_*

      - name: Restore listings hash cache (DE)
        uses: actions/cache@v4
        with:
          path: .cache
          key: listings-hash-de-${{ github.ref_name }}
          restore-keys: |
            listings-hash-de-

      - name: Check if listings JSON changed (DE)
        id: check_de
        run: |
          set -e
          mkdir -p .cache
          FILE=amazon_de_listings.json
          if [ ! -f "$FILE" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          NEW_HASH=$(md5sum "$FILE" | awk '{print $1}')
          OLD_HASH=""
          [ -f .cache/amazon_de_listings.md5 ] && OLD_HASH=$(cat .cache/amazon_de_listings.md5 || true)
          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -n "$NEW_HASH" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "$NEW_HASH" > .cache/amazon_de_listings.md5
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Send Amazon DE Listings JSON feed (SP-API)
        if: steps.check_de.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          OUT=$(python publish_spapi_feed.py \
            --file amazon_de_listings.json \
            --feed-type JSON_LISTINGS_FEED \
            --marketplace A1PA6795UKMFR9)
          echo "$OUT"
          FEED_ID=$(echo "$OUT" | grep -o '"feedId": *"[0-9]*"' | grep -o '[0-9]*' || true)
          test -n "$FEED_ID"
          echo "FEED_ID_DE=$FEED_ID" >> $GITHUB_ENV

      - name: Check feed processing status (DE)
        if: steps.check_de.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          sleep 20
          python check_spapi_feed.py --feed-id "$FEED_ID_DE"
          mv -f feed_status.json feed_status_de.json || true
          mv -f feed_processing_report.txt feed_processing_report_de.txt || true
          mv -f feed_document_meta.json feed_document_meta_de.json || true

      - name: Upload artifacts (DE)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: de-feed-artifacts
          path: |
            amazon_de_listings.json
            amazon_de_b2c.csv
            amazon_de_b2b.csv
            feed_status_de.json
            feed_processing_report_de.txt
            feed_document_meta_de.json
          if-no-files-found: warn
          retention-days: 7

      - name: Fail if feed report contains errors (DE)
        if: steps.check_de.outputs.changed == 'true'
        run: |
          set -e
          python - << 'PY'
          import json, sys, pathlib
          p = pathlib.Path("feed_processing_report_de.txt")
          if not p.exists():
              print("Missing report -> fail")
              sys.exit(1)
          obj = json.loads(p.read_text(encoding="utf-8", errors="replace").strip())
          errors = int((obj.get("summary") or {}).get("errors") or 0)
          print("Report summary DE errors=", errors)
          if errors > 0:
              print("Issues:", obj.get("issues", [])[:50])
              sys.exit(1)
          PY

      - name: Upload amazon_de_listings.json to Drive (backup)
        if: always()
        env:
          FILE_ID: ${{ secrets.AMAZON_DE_LISTINGS_JSON_FILE_ID }}
          LOCAL_FILE: amazon_de_listings.json
        run: |
          set -e
          test -n "$FILE_ID"
          test -f "$LOCAL_FILE"
          python upload_to_drive.py


      # ---------- ES ----------
      - name: Generate Amazon feeds ES
        env:
          GSHEET_ID: ${{ secrets.GSHEET_ID }}
          COUNTRY: es
          AMAZON_SELLER_ID: ${{ secrets.AMAZON_SELLER_ID }}
        run: |
          set -e
          python generate_amazon_feeds.py
          ls -lh amazon_es_*

      - name: Restore listings hash cache (ES)
        uses: actions/cache@v4
        with:
          path: .cache
          key: listings-hash-es-${{ github.ref_name }}
          restore-keys: |
            listings-hash-es-

      - name: Check if listings JSON changed (ES)
        id: check_es
        run: |
          set -e
          mkdir -p .cache
          FILE=amazon_es_listings.json
          if [ ! -f "$FILE" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          NEW_HASH=$(md5sum "$FILE" | awk '{print $1}')
          OLD_HASH=""
          [ -f .cache/amazon_es_listings.md5 ] && OLD_HASH=$(cat .cache/amazon_es_listings.md5 || true)
          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -n "$NEW_HASH" ]; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "$NEW_HASH" > .cache/amazon_es_listings.md5
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Send Amazon ES Listings JSON feed (SP-API)
        if: steps.check_es.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          OUT=$(python publish_spapi_feed.py \
            --file amazon_es_listings.json \
            --feed-type JSON_LISTINGS_FEED \
            --marketplace A1RKKUPIHCS9HS)
          echo "$OUT"
          FEED_ID=$(echo "$OUT" | grep -o '"feedId": *"[0-9]*"' | grep -o '[0-9]*' || true)
          test -n "$FEED_ID"
          echo "FEED_ID_ES=$FEED_ID" >> $GITHUB_ENV

      - name: Check feed processing status (ES)
        if: steps.check_es.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          sleep 20
          python check_spapi_feed.py --feed-id "$FEED_ID_ES"
          mv -f feed_status.json feed_status_es.json || true
          mv -f feed_processing_report.txt feed_processing_report_es.txt || true
          mv -f feed_document_meta.json feed_document_meta_es.json || true

      - name: Upload artifacts (ES)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: es-feed-artifacts
          path: |
            amazon_es_listings.json
            amazon_es_b2c.csv
            amazon_es_b2b.csv
            feed_status_es.json
            feed_processing_report_es.txt
            feed_document_meta_es.json
          if-no-files-found: warn
          retention-days: 7

      - name: Fail if feed report contains errors (ES)
        if: steps.check_es.outputs.changed == 'true'
        run: |
          set -e
          python - << 'PY'
          import json, sys, pathlib
          p = pathlib.Path("feed_processing_report_es.txt")
          if not p.exists():
              print("Missing report -> fail")
              sys.exit(1)
          obj = json.loads(p.read_text(encoding="utf-8", errors="replace").strip())
          errors = int((obj.get("summary") or {}).get("errors") or 0)
          print("Report summary ES errors=", errors)
          if errors > 0:
              print("Issues:", obj.get("issues", [])[:50])
              sys.exit(1)
          PY

      - name: Upload amazon_es_listings.json to Drive (backup)
        if: always()
        env:
          FILE_ID: ${{ secrets.AMAZON_ES_LISTINGS_JSON_FILE_ID }}
          LOCAL_FILE: amazon_es_listings.json
        run: |
          set -e
          test -n "$FILE_ID"
          test -f "$LOCAL_FILE"
          python upload_to_drive.py
