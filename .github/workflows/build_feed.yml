name: Build filtered CSV feed + Amazon feeds (multi-country)

on:
  schedule:
    - cron: "*/15 * * * *"     # filtro + diff + amazon feeds
    - cron: "5 0 * * *"        # baseline giornaliera (UTC)
  workflow_dispatch:

concurrency:
  group: feed-filter
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Debug repo files
        run: |
          set -e
          echo "PWD=$(pwd)"
          ls -lah
          echo "Find key scripts:"
          find . -maxdepth 2 -type f -name "generate_amazon_feeds.py" -o -name "filter_feed.py" -o -name "upload_to_drive.py" -o -name "publish_spapi_feed.py" -print

      # ===============================
      # INSTALL DEPS
      # ===============================
      - name: Install Python deps (Google + SP-API)
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install google-api-python-client google-auth google-auth-httplib2
          pip install requests botocore

      # ===============================
      # DOWNLOAD SOURCE
      # ===============================
      - name: Download source CSV
        run: |
          set -e
          echo "Downloading source..."
          curl -L --retry 5 --retry-delay 5 --connect-timeout 30 --max-time 3600 \
            -w "\nHTTP_CODE=%{http_code}\nFINAL_URL=%{url_effective}\n" \
            "${{ secrets.SOURCE_CSV_URL }}" -o source.csv
          echo "Downloaded file:"
          ls -lh source.csv
          echo "Header preview:"
          head -n 2 source.csv | cat

      # ===============================
      # FILTER
      # ===============================
      - name: Filter CSV
        run: |
          set -e
          python filter_feed.py
          echo "Filtered file:"
          ls -lh filtered.csv
          echo "Filtered header preview:"
          head -n 2 filtered.csv | cat

      # ===============================
      # SERVICE ACCOUNT (Drive + Sheets)
      # ===============================
      - name: Write service account JSON to file
        env:
          GDRIVE_SA_JSON: ${{ secrets.GDRIVE_SA_JSON }}
        run: |
          set -e
          if [ -z "$GDRIVE_SA_JSON" ]; then
            echo "GDRIVE_SA_JSON secret is empty or missing"
            exit 1
          fi
          echo "$GDRIVE_SA_JSON" > sa.json
          python - << 'PY'
          import json
          with open("sa.json","r",encoding="utf-8") as f:
              obj = json.load(f)
          print("SA client_email:", obj.get("client_email"))
          PY

      # ===============================
      # UPLOAD filtered.csv (overwrite)
      # ===============================
      - name: Upload filtered.csv to Google Drive (overwrite by fileId)
        env:
          GDRIVE_FILE_ID: ${{ secrets.GDRIVE_FILE_ID }}
        run: |
          set -e
          python - << 'PY'
          import os
          from google.oauth2 import service_account
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload

          file_id = os.environ.get("GDRIVE_FILE_ID","").strip()
          if not file_id:
              raise RuntimeError("GDRIVE_FILE_ID is empty or missing")

          creds = service_account.Credentials.from_service_account_file(
              "sa.json",
              scopes=["https://www.googleapis.com/auth/drive"]
          )
          service = build("drive", "v3", credentials=creds)
          media = MediaFileUpload("filtered.csv", mimetype="text/csv", resumable=True)
          updated = service.files().update(fileId=file_id, media_body=media).execute()
          print("Updated Drive file:", updated.get("id"))
          PY

      # ===============================
      # BASELINE: daily reset (UTC)
      # ===============================
      - name: Build baseline_skus.txt (daily)
        if: github.event_name == 'schedule' && github.event.schedule == '5 0 * * *'
        run: |
          set -e
          python - << 'PY'
          import csv
          from pathlib import Path

          def detect_delim(text: str) -> str:
              sample = text[:8192]
              try:
                  d = csv.Sniffer().sniff(sample, delimiters=",;\t|")
                  return d.delimiter
              except Exception:
                  first = text.splitlines()[0] if text else ""
                  return ";" if first.count(";") > first.count(",") else ","

          def find_col(header_l, candidates):
              norm = [h.replace(" ", "_") for h in header_l]
              for c in candidates:
                  c0 = c.lower().replace(" ", "_")
                  for i, h in enumerate(norm):
                      if h == c0:
                          return i
                  for i, h in enumerate(norm):
                      if c0 in h:
                          return i
              return -1

          raw = Path("filtered.csv").read_bytes()
          text = raw.decode("utf-8-sig", errors="replace")
          delim = detect_delim(text)
          reader = csv.reader(text.splitlines(), delimiter=delim)

          header = next(reader, [])
          header_l = [h.strip().lower() for h in header]
          sku_i = find_col(header_l, ["sku","seller_sku","seller sku","item_sku","item sku","merchant_sku","merchant sku","sku_id"])
          if sku_i < 0:
              raise RuntimeError(f"SKU column not found. Detected delimiter={delim!r}. Header={header}")

          seen = set()
          for row in reader:
              if not row or sku_i >= len(row):
                  continue
              sku = (row[sku_i] or "").strip()
              if sku:
                  seen.add(sku)

          with open("baseline_skus.txt","w",encoding="utf-8",newline="\n") as out:
              for sku in sorted(seen):
                  out.write(sku + "\n")

          print("Detected delimiter:", repr(delim))
          print("Baseline SKUs:", len(seen))
          PY

      - name: Upload baseline_skus.txt to Google Drive (overwrite by fileId)
        if: github.event_name == 'schedule' && github.event.schedule == '5 0 * * *'
        env:
          FILE_ID: ${{ secrets.BASELINE_FILE_ID }}
          LOCAL_FILE: baseline_skus.txt
        run: |
          set -e
          python upload_to_drive.py

      # ===============================
      # DIFF: every 15 min (and manual)
      # ===============================
      - name: Download baseline_skus.txt from Google Drive
        if: github.event_name != 'schedule' || github.event.schedule != '5 0 * * *'
        env:
          BASELINE_FILE_ID: ${{ secrets.BASELINE_FILE_ID }}
        run: |
          set -e
          python - << 'PY'
          import os
          from google.oauth2 import service_account
          from googleapiclient.discovery import build

          file_id = os.environ.get("BASELINE_FILE_ID","").strip()
          if not file_id:
              raise RuntimeError("BASELINE_FILE_ID is empty or missing")

          creds = service_account.Credentials.from_service_account_file(
              "sa.json",
              scopes=["https://www.googleapis.com/auth/drive"]
          )
          service = build("drive", "v3", credentials=creds)

          data = service.files().get_media(fileId=file_id).execute()
          with open("baseline_skus.txt","wb") as f:
              f.write(data)
          print("Downloaded baseline_skus.txt bytes:", len(data))
          PY

      - name: Build new_skus_0930.csv (diff vs baseline)
        if: github.event_name != 'schedule' || github.event.schedule != '5 0 * * *'
        run: |
          set -e
          python - << 'PY'
          import csv
          from pathlib import Path

          def detect_delim(text: str) -> str:
              sample = text[:8192]
              try:
                  d = csv.Sniffer().sniff(sample, delimiters=",;\t|")
                  return d.delimiter
              except Exception:
                  first = text.splitlines()[0] if text else ""
                  return ";" if first.count(";") > first.count(",") else ","

          def find_col(header_l, candidates):
              norm = [h.replace(" ", "_") for h in header_l]
              for c in candidates:
                  c0 = c.lower().replace(" ", "_")
                  for i, h in enumerate(norm):
                      if h == c0:
                          return i
                  for i, h in enumerate(norm):
                      if c0 in h:
                          return i
              return -1

          baseline = set()
          with open("baseline_skus.txt","r",encoding="utf-8",errors="ignore") as f:
              for line in f:
                  s = line.strip()
                  if s:
                      baseline.add(s)

          raw = Path("filtered.csv").read_bytes()
          text = raw.decode("utf-8-sig", errors="replace")
          delim = detect_delim(text)

          reader = csv.reader(text.splitlines(), delimiter=delim)
          header = next(reader, [])
          header_l = [h.strip().lower() for h in header]

          sku_i = find_col(header_l, ["sku","seller_sku","seller sku","item_sku","item sku","merchant_sku","merchant sku","sku_id"])
          supplier_i = find_col(header_l, ["supplier_code","supplier","supplierid","supplier_id"])
          name_i = find_col(header_l, ["titolo_prodotto","name","title","product_name"])
          qty_i = find_col(header_l, ["quantita","qty","quantity","stock"])
          cat_i = find_col(header_l, ["cat1","category","categoria"])
          cost_i = find_col(header_l, ["prezzo_iva_esclusa","cost","price","base_price","prezzo","net_price"])

          # âœ… EAN / barcode / gtin
          ean_i = find_col(header_l, ["ean","ean_code","barcode","gtin","gtin13","ean13","product_ean","product_ean_code"])

          if sku_i < 0:
              raise RuntimeError(f"SKU column not found. Detected delimiter={delim!r}. Header={header}")

          out_header = ["sku","ean","supplier_code","name","qty","category","cost"]
          rows_out = 0

          def get(row, i):
              return (row[i] if i >= 0 and i < len(row) else "").strip()

          with open("new_skus_0930.csv","w",encoding="utf-8",newline="") as fout:
              w = csv.writer(fout)
              w.writerow(out_header)

              for row in reader:
                  if not row or sku_i >= len(row):
                      continue
                  sku = get(row, sku_i)
                  if not sku or sku in baseline:
                      continue

                  supplier = get(row, supplier_i)
                  if not supplier:
                      parts = sku.split("_")
                      supplier = parts[1] if len(parts) >= 2 else ""

                  ean = get(row, ean_i)

                  w.writerow([sku, ean, supplier, get(row, name_i), get(row, qty_i), get(row, cat_i), get(row, cost_i)])
                  rows_out += 1

          print("Detected delimiter:", repr(delim))
          print("Baseline size:", len(baseline))
          print("EAN column index:", ean_i)
          print("New SKUs rows:", rows_out)
          PY

      - name: Upload new_skus_0930.csv to Google Drive (overwrite by fileId)
        if: github.event_name != 'schedule' || github.event.schedule != '5 0 * * *'
        env:
          FILE_ID: ${{ secrets.NEWSKUS_FILE_ID }}
          LOCAL_FILE: new_skus_0930.csv
        run: |
          set -e
          python upload_to_drive.py

      # ===============================
      # GENERATE AMAZON FEEDS
      # ===============================
      # ===============================
      # GENERATE AMAZON FEEDS
      # ===============================

      - name: Generate Amazon feeds IT
        env:
          GSHEET_ID: ${{ secrets.GSHEET_ID }}
          COUNTRY: it
          AMAZON_SELLER_ID: ${{ secrets.AMAZON_SELLER_ID }}
        run: |
          set -e
          python generate_amazon_feeds.py
          ls -lh amazon_it_*

      # ===============================
      # CHANGE DETECTION (persisted via cache)
      # ===============================

      - name: Restore listings hash cache
        id: cache_listings_hash
        uses: actions/cache@v4
        with:
          path: .cache
          key: listings-hash-it-${{ github.ref_name }}
          restore-keys: |
            listings-hash-it-

      - name: Check if listings JSON changed
        id: check_json
        run: |
          set -e
          mkdir -p .cache

          if [ ! -f amazon_it_listings.json ]; then
            echo "amazon_it_listings.json not found -> treat as unchanged=false? (skip)"
            echo "changed=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          NEW_HASH=$(md5sum amazon_it_listings.json | awk '{print $1}')
          echo "NEW_HASH=$NEW_HASH"

          OLD_HASH=""
          if [ -f .cache/amazon_it_listings.md5 ]; then
            OLD_HASH=$(cat .cache/amazon_it_listings.md5 || true)
          fi
          echo "OLD_HASH=$OLD_HASH"

          if [ "$NEW_HASH" = "$OLD_HASH" ] && [ -n "$NEW_HASH" ]; then
            echo "No changes detected -> skip send."
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "$NEW_HASH" > .cache/amazon_it_listings.md5
            echo "Changes detected -> will send."
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      # (optional) debug
      - name: Debug change flag
        run: |
          echo "changed = ${{ steps.check_json.outputs.changed }}"

      # ===============================
      # SEND JSON_LISTINGS_FEED (SP-API)
      # ===============================

      - name: Send Amazon IT Listings JSON feed (SP-API)
        if: steps.check_json.outputs.changed == 'true'
        id: send_feed
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          FEED_OUTPUT=$(python publish_spapi_feed.py \
            --file amazon_it_listings.json \
            --feed-type JSON_LISTINGS_FEED \
            --marketplace APJ6JRA9NG5V4)

          echo "$FEED_OUTPUT"

          FEED_ID=$(echo "$FEED_OUTPUT" | grep -o '"feedId": *"[0-9]*"' | grep -o '[0-9]*' || true)
          if [ -z "$FEED_ID" ]; then
            echo "ERROR: cannot parse feedId from output"
            exit 1
          fi

          echo "Feed ID: $FEED_ID"
          echo "FEED_ID=$FEED_ID" >> $GITHUB_ENV

      # ===============================
      # WAIT BEFORE STATUS CHECK
      # ===============================

      - name: Wait before checking feed status
        if: steps.check_json.outputs.changed == 'true'
        run: sleep 20

      # ===============================
      # CHECK FEED PROCESSING STATUS
      # ===============================

      - name: Check feed processing status
        if: steps.check_json.outputs.changed == 'true'
        env:
          SPAPI_LWA_CLIENT_ID: ${{ secrets.SPAPI_LWA_CLIENT_ID }}
          SPAPI_LWA_CLIENT_SECRET: ${{ secrets.SPAPI_LWA_CLIENT_SECRET }}
          SPAPI_LWA_REFRESH_TOKEN: ${{ secrets.SPAPI_LWA_REFRESH_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          set -e
          python check_spapi_feed.py --feed-id $FEED_ID

          echo "=== feed_status.json ==="
          cat feed_status.json || true

          echo "=== feed_processing_report.txt (head) ==="
          head -n 120 feed_processing_report.txt || true

      # ===============================
      # UPLOAD ARTIFACTS (ALWAYS)
      # ===============================

      - name: Upload feed status (always)
        if: steps.check_json.outputs.changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: feed-status
          path: feed_status.json
          if-no-files-found: warn
          retention-days: 7

      - name: Upload feed processing report (always)
        if: steps.check_json.outputs.changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: feed-processing-report
          path: feed_processing_report.txt
          if-no-files-found: warn
          retention-days: 7

      - name: Upload feed document meta (always)
        if: steps.check_json.outputs.changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: feed-document-meta
          path: feed_document_meta.json
          if-no-files-found: warn
          retention-days: 7

      # ===============================
      # FAIL WORKFLOW IF REPORT HAS ERRORS
      # ===============================

      - name: Fail if feed report contains errors
        if: steps.check_json.outputs.changed == 'true'
        run: |
          set -e
          if [ ! -f feed_processing_report.txt ]; then
            echo "No feed_processing_report.txt found -> cannot validate. Failing."
            exit 1
          fi

          python - << 'PY'
          import json, sys
          p="feed_processing_report.txt"
          txt=open(p,"r",encoding="utf-8",errors="replace").read().strip()

          try:
            obj=json.loads(txt)
          except Exception:
            if "ERROR" in txt.upper():
              print("Report is not JSON but contains 'ERROR' -> FAIL")
              sys.exit(1)
            print("Report is not JSON and no 'ERROR' found -> OK")
            sys.exit(0)

          summary=obj.get("summary") or {}
          errors=int(summary.get("errors") or 0)
          warnings=int(summary.get("warnings") or 0)

          print(f"Report summary: errors={errors}, warnings={warnings}")

          issues=obj.get("issues") or []
          if issues:
            print("Issues:")
            for it in issues[:50]:
              print(it)

          if errors > 0:
            sys.exit(1)
          PY
      
      # ===============================
      # UPLOAD AMAZON FEEDS TO DRIVE (overwrite) - keep as backup
      # ===============================
      - name: Upload amazon_it_price_quantity.txt to Drive
        env:
          FILE_ID: ${{ secrets.AMAZON_IT_PRICE_QTY_FILE_ID }}
          LOCAL_FILE: amazon_it_price_quantity.txt
        run: |
          set -e
          python upload_to_drive.py

      - name: Upload amazon_it_business_pricing.txt to Drive
        if: false
        env:
          FILE_ID: ${{ secrets.AMAZON_IT_BUSINESS_FILE_ID }}
          LOCAL_FILE: amazon_it_business_pricing.txt
        run: |
          set -e
          python upload_to_drive.py

      # ===============================
      # ARTIFACTS (debug)
      # ===============================
      - name: Upload artifact (debug pipeline files)
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-files
          path: |
            filtered.csv
            new_skus_0930.csv
          if-no-files-found: warn
          retention-days: 3

      - name: Upload artifact (debug amazon feeds)
        uses: actions/upload-artifact@v4
        with:
          name: amazon-feeds
          path: |
            amazon_it_price_quantity.txt
            amazon_it_business_pricing.txt
          if-no-files-found: warn
          retention-days: 3
